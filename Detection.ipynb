{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vehicle detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import glob\n",
    "import os.path\n",
    "import pickle\n",
    "import time\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from skimage.feature import hog\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.ndimage.measurements import label\n",
    "\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "color_space = 'YCrCb' # RGB, HSV, LUV, HLS, YUV or YCrCb\n",
    "\n",
    "## HOG\n",
    "hog_feat = True # HOG features on or off\n",
    "orient = 9 # number of orientation bins\n",
    "pix_per_cell = 8 # size of a cell in pixels\n",
    "cell_per_block = 2 # number of cells per block\n",
    "hog_channel = 'ALL' # image channel to include. Can be 0, 1, 2, or \"ALL\"\n",
    "\n",
    "## Spatial\n",
    "spatial_feat = True # Spatial features on or off\n",
    "spatial_size = (32, 32) # Spatial binning dimensions\n",
    "\n",
    "## Histogram\n",
    "hist_feat = True # Histogram features on or off\n",
    "hist_bins = 32    # Number of histogram bins\n",
    "\n",
    "## Misc\n",
    "window_size = (64, 64) # default sliding window size\n",
    "y_start_stop = [390, 670] # min and max y coordinate to search in slide_window()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bin_spatial(img, size=spatial_size):\n",
    "    \"\"\"\n",
    "    Get a vector of spatial features. \n",
    "    \n",
    "    This is just the raw pixels in a resized version of the original image.\n",
    "    \"\"\"\n",
    "    features = cv2.resize(img, size).ravel() \n",
    "    return features\n",
    "\n",
    "def color_hist(img, nbins=hist_bins, bins_range=(0, 256)):\n",
    "    \"\"\"\n",
    "    Get a vector containing histograms of the color channels in the image.\n",
    "    \"\"\"\n",
    "    # Compute the histogram of the color channels separately\n",
    "    channel1_hist = np.histogram(img[:,:,0], bins=nbins, range=bins_range)\n",
    "    channel2_hist = np.histogram(img[:,:,1], bins=nbins, range=bins_range)\n",
    "    channel3_hist = np.histogram(img[:,:,2], bins=nbins, range=bins_range)\n",
    "    # Concatenate the histograms into a single feature vector\n",
    "    hist_features = np.concatenate((channel1_hist[0], channel2_hist[0], channel3_hist[0]))\n",
    "    return hist_features\n",
    "\n",
    "def get_hog_features(img, orient=orient, pix_per_cell=pix_per_cell, \n",
    "                     cell_per_block=cell_per_block, \n",
    "                     vis=False, feature_vec=True):\n",
    "    \"\"\"\n",
    "    Return HOG features and optionally a visualization.\n",
    "    \"\"\"\n",
    "    # Call with two outputs if vis==True\n",
    "    if vis == True:\n",
    "        features, hog_image = hog(img, orientations=orient, \n",
    "                                  pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                                  cells_per_block=(cell_per_block, cell_per_block), \n",
    "                                  transform_sqrt=True, \n",
    "                                  visualise=vis, feature_vector=feature_vec)\n",
    "        return features, hog_image\n",
    "    # Otherwise call with one output\n",
    "    else:      \n",
    "        features = hog(img, orientations=orient, \n",
    "                       pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                       cells_per_block=(cell_per_block, cell_per_block), \n",
    "                       transform_sqrt=True, \n",
    "                       visualise=vis, feature_vector=feature_vec)\n",
    "        return features\n",
    "    \n",
    "def convert_color(img, color_space):\n",
    "    \"\"\"\n",
    "    Convert image to `color_space`.\n",
    "    \n",
    "    Image is assumed to be in RGB.\n",
    "    `color_space` should be one of HSV, LUV, HLS, YUV or YCrCb.\n",
    "    \"\"\"\n",
    "    if color_space == 'HSV':\n",
    "        return cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "    elif color_space == 'LUV':\n",
    "        return cv2.cvtColor(img, cv2.COLOR_RGB2LUV)\n",
    "    elif color_space == 'HLS':\n",
    "        return cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "    elif color_space == 'YUV':\n",
    "        return cv2.cvtColor(img, cv2.COLOR_RGB2YUV)\n",
    "    elif color_space == 'YCrCb':\n",
    "        return cv2.cvtColor(img, cv2.COLOR_RGB2YCrCb)\n",
    "    else:\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(img, color_space=color_space, spatial_size=spatial_size,\n",
    "                     hist_bins=hist_bins, orient=orient, \n",
    "                     pix_per_cell=pix_per_cell, cell_per_block=cell_per_block, \n",
    "                     hog_channel=hog_channel, spatial_feat=spatial_feat, \n",
    "                     hist_feat=hist_feat, hog_feat=hog_feat, precomputed_hog_features=None):\n",
    "    \"\"\"\n",
    "    Extract features from a single image.\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(img, str):\n",
    "        img = read_img(img)\n",
    "    \n",
    "    #1) Define an empty list to receive features\n",
    "    img_features = []\n",
    "    #2) Apply color conversion if other than 'RGB'\n",
    "    feature_image = convert_color(img, color_space)\n",
    "    #3) Compute spatial features if flag is set\n",
    "    if spatial_feat == True:\n",
    "        spatial_features = bin_spatial(feature_image, size=spatial_size)\n",
    "        #4) Append features to list\n",
    "        img_features.append(spatial_features)\n",
    "    #5) Compute histogram features if flag is set\n",
    "    if hist_feat == True:\n",
    "        hist_features = color_hist(feature_image, nbins=hist_bins)\n",
    "        #6) Append features to list\n",
    "        img_features.append(hist_features)\n",
    "    #7) Compute HOG features if flag is set\n",
    "    if hog_feat == True:\n",
    "        if precomputed_hog_features is not None:\n",
    "            hog_features = precomputed_hog_features\n",
    "        elif hog_channel == 'ALL':\n",
    "            hog_features = []\n",
    "            for channel in range(feature_image.shape[2]):\n",
    "                hog_features.extend(get_hog_features(feature_image[:,:,channel], \n",
    "                                    orient, pix_per_cell, cell_per_block, \n",
    "                                    vis=False, feature_vec=True))      \n",
    "        else:\n",
    "            hog_features = get_hog_features(feature_image[:,:,hog_channel], orient, \n",
    "                        pix_per_cell, cell_per_block, vis=False, feature_vec=True)\n",
    "        #8) Append features to list\n",
    "        img_features.append(hog_features)\n",
    "\n",
    "    #9) Return concatenated array of features\n",
    "    return np.concatenate(img_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images = glob.glob('data/**/*.png', recursive=True)\n",
    "cars = []\n",
    "notcars = []\n",
    "\n",
    "for image in images:\n",
    "    if 'non-vehicles' in image:\n",
    "        notcars.append(image)\n",
    "    else:\n",
    "        cars.append(image)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_img(fpath):\n",
    "    img = mpimg.imread(fpath)\n",
    "    if '.png' in fpath:\n",
    "        return (img*255).astype(np.uint8)\n",
    "    else:\n",
    "        return img.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split into training and test sets\n",
    "cars_train, cars_test = train_test_split(cars)\n",
    "notcars_train, notcars_test = train_test_split(notcars)\n",
    "\n",
    "augmentation_factor = 0 # optionally, augment training data with randomly \"jittered\" examples\n",
    "\n",
    "X_train = []\n",
    "for img in np.concatenate((cars_train, notcars_train)):\n",
    "    img = read_img(img)\n",
    "    X_train.append(extract_features(img))\n",
    "    for _ in range(augmentation_factor):\n",
    "        X_train.append(extract_features(augmentation_pipeline(img)))\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.concatenate((np.ones(len(cars_train)*(1+augmentation_factor), np.uint8), \n",
    "                          np.zeros(len(notcars_train)*(1+augmentation_factor), np.uint8)))\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "X_test = []\n",
    "for img in np.concatenate((cars_test, notcars_test)):\n",
    "    X_test.append(extract_features(img))\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.concatenate((np.ones_like(cars_test, np.uint8), np.zeros_like(notcars_test, np.uint8)))\n",
    "X_test, y_test = shuffle(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use a scaler to normalize data\n",
    "scaler = RobustScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9982\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('lsvcclassifier.p'):\n",
    "    lsvc = pickle.load(open('lsvcclassifier.p', 'rb'))\n",
    "else:\n",
    "    lsvc = LinearSVC(C=0.25)\n",
    "    lsvc.fit(X_train, y_train)\n",
    "    pickle.dump(lsvc, open('lsvcclassifier.p', 'wb'))\n",
    "\n",
    "svc = lsvc\n",
    "print(\"Accuracy: {:.4f}\".format(lsvc.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9953\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('adaboostclassifier.p'):\n",
    "    booster = pickle.load(open('adaboostclassifier.p', 'rb'))\n",
    "else:\n",
    "    booster = AdaBoostClassifier()\n",
    "    booster.fit(X_train, y_train)\n",
    "    pickle.dump(booster, open('adaboostclassifier.p', 'wb'))\n",
    "\n",
    "print(\"Accuracy: {:.4f}\".format(booster.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "forest = RandomForestClassifier(n_estimators=10, n_jobs=-1, min_samples_split=10, max_depth=5)\n",
    "forest.fit(X_train, y_train)\n",
    "forest.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "svc = SVC(kernel='poly')\n",
    "svc.fit(X_train, y_train)\n",
    "svc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def slide_window(img, x_start_stop=[None, None], y_start_stop=y_start_stop, \n",
    "                    xy_window=window_size, xy_overlap=(0.5, 0.5)):\n",
    "    \"\"\"\n",
    "    Return a list of sliding windows (boxes) given x and y ranges, size of window\n",
    "    and the percentage overlap between consecutive windows.\n",
    "    \"\"\"\n",
    "    # If x and/or y start/stop positions not defined, set to image size\n",
    "    if x_start_stop[0] == None:\n",
    "        x_start_stop[0] = 0\n",
    "    if x_start_stop[1] == None:\n",
    "        x_start_stop[1] = img.shape[1]\n",
    "    if y_start_stop[0] == None:\n",
    "        y_start_stop[0] = 0\n",
    "    if y_start_stop[1] == None:\n",
    "        y_start_stop[1] = img.shape[0]\n",
    "    # Compute the span of the region to be searched    \n",
    "    xspan = x_start_stop[1] - x_start_stop[0]\n",
    "    yspan = y_start_stop[1] - y_start_stop[0]\n",
    "    # Compute the number of pixels per step in x/y\n",
    "    nx_pix_per_step = np.int(xy_window[0]*(1 - xy_overlap[0]))\n",
    "    ny_pix_per_step = np.int(xy_window[1]*(1 - xy_overlap[1]))\n",
    "    # Compute the number of windows in x/y\n",
    "    nx_buffer = np.int(xy_window[0]*(xy_overlap[0]))\n",
    "    ny_buffer = np.int(xy_window[1]*(xy_overlap[1]))\n",
    "    nx_windows = np.int((xspan-nx_buffer)/nx_pix_per_step) \n",
    "    ny_windows = np.int((yspan-ny_buffer)/ny_pix_per_step) \n",
    "    # Initialize a list to append window positions to\n",
    "    window_list = []\n",
    "    # Loop through finding x and y window positions\n",
    "    # Note: you could vectorize this step, but in practice\n",
    "    # you'll be considering windows one by one with your\n",
    "    # classifier, so looping makes sense\n",
    "    for ys in range(ny_windows):\n",
    "        for xs in range(nx_windows):\n",
    "            # Calculate window position\n",
    "            startx = xs*nx_pix_per_step + x_start_stop[0]\n",
    "            endx = startx + xy_window[0]\n",
    "            starty = ys*ny_pix_per_step + y_start_stop[0]\n",
    "            endy = starty + xy_window[1]\n",
    "            \n",
    "            # Append window position to list\n",
    "            window_list.append(((startx, starty), (endx, endy)))\n",
    "    # Return the list of windows\n",
    "    return window_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_windows(img, windows, classifiers, scaler, color_space=color_space, \n",
    "                    spatial_size=spatial_size, hist_bins=hist_bins, \n",
    "                    hist_range=(0, 256), orient=orient, \n",
    "                    pix_per_cell=pix_per_cell, cell_per_block=cell_per_block, \n",
    "                    hog_channel=hog_channel, spatial_feat=spatial_feat, \n",
    "                    hist_feat=hist_feat, hog_feat=hog_feat, window_size = window_size):\n",
    "    \"\"\"\n",
    "    Run a prediction for a list of window coordinates. \n",
    "    \n",
    "    Returns windows for which all classifiers returned a positive result.\n",
    "    \"\"\"\n",
    "    #1) Create an empty list to receive positive detection windows\n",
    "    on_windows = []\n",
    "   \n",
    "    #2) Iterate over all windows in the list\n",
    "    for window in windows:\n",
    "        #3) Extract the test window from original image\n",
    "        test_img = cv2.resize(img[window[0][1]:window[1][1], window[0][0]:window[1][0]], (64, 64)) \n",
    "        #4) Extract features for that window\n",
    "        features = extract_features(test_img, color_space=color_space, \n",
    "                            spatial_size=spatial_size, hist_bins=hist_bins, \n",
    "                            orient=orient, pix_per_cell=pix_per_cell, \n",
    "                            cell_per_block=cell_per_block, \n",
    "                            hog_channel=hog_channel, spatial_feat=spatial_feat, \n",
    "                            hist_feat=hist_feat, hog_feat=hog_feat)\n",
    "        #5) Scale extracted features to be fed to classifiers\n",
    "        test_features = scaler.transform(np.array(features).reshape(1, -1))\n",
    "        #6) Predict using the classifiers\n",
    "        predictions = [clf.predict(test_features) for clf in classifiers]\n",
    "        #7) If positive (all predictions == True) then save the window\n",
    "        if all(predictions):\n",
    "            on_windows.append(window)\n",
    "    #8) Return windows for positive detections\n",
    "    return on_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_labeled_bboxes(img, labels):\n",
    "    \"\"\"\n",
    "    Draw boxes given a (car) labeled array.\n",
    "    \"\"\"\n",
    "    # Iterate through all detected cars\n",
    "    for car_number in range(1, labels[1]+1):\n",
    "        # Find pixels with each car_number label value\n",
    "        nonzero = (labels[0] == car_number).nonzero()\n",
    "        # Identify x and y values of those pixels\n",
    "        nonzeroy = np.array(nonzero[0])\n",
    "        nonzerox = np.array(nonzero[1])\n",
    "        # Define a bounding box based on min/max x and y\n",
    "        bbox = ((np.min(nonzerox), np.min(nonzeroy)), (np.max(nonzerox), np.max(nonzeroy)))\n",
    "        # If predicted box sufficiently wide draw the box on the image\n",
    "        if bbox[1][0] - bbox[0][0] > 100:\n",
    "            cv2.rectangle(img, bbox[0], bbox[1], (0,0,255), 6)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video/sequence processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RunningHeatmap:\n",
    "    \"\"\"\n",
    "    Container for a heatmap formed by a sequence of box predictions.\n",
    "    \"\"\"\n",
    "    def __init__(self, shape, history=60, threshold=3):\n",
    "        self.history = history\n",
    "        self.threshold = threshold\n",
    "        self._heatmap = np.zeros(shape, np.uint32)\n",
    "        self._boxes = []\n",
    "        \n",
    "    def add(self, boxes):\n",
    "        \"\"\"\n",
    "        Add a list of boxes to the running heatmap.\n",
    "        \n",
    "        Method automatically removes old boxes if max history exceeded.\n",
    "        \"\"\"\n",
    "        self._boxes.append(boxes)\n",
    "        if len(self._boxes) > self.history:\n",
    "            popped = self._boxes.pop(0)\n",
    "            for box in popped:\n",
    "                self._heatmap[box[0][1]:box[1][1], box[0][0]:box[1][0]] -= 1\n",
    "        \n",
    "        for box in boxes:\n",
    "            self._heatmap[box[0][1]:box[1][1], box[0][0]:box[1][0]] += 1\n",
    "            \n",
    "    @property\n",
    "    def heatmap(self):\n",
    "        \"\"\"\n",
    "        Get the current tresholded value of the heatmap.\n",
    "        \"\"\"\n",
    "        threshold = max(min(self.threshold, len(self._boxes)//2), 3)\n",
    "        ret = np.clip(self._heatmap, 0, 255).astype(np.uint8)\n",
    "        ret[ret <= self.threshold] = 0\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Centroid:\n",
    "    \"\"\"\n",
    "    Container class which tracks an object via its centroid.\n",
    "    \n",
    "    New detections which fall within the range of this object are\n",
    "    considered to be a detection of the same object.\n",
    "    \"\"\"\n",
    "    MAX_PIXEL_DISTANCE = 100\n",
    "    \n",
    "    def __init__(self, box):\n",
    "        self.previous_box = box\n",
    "        self.box = box\n",
    "        self.draw_box = box\n",
    "        self.activations = [True]\n",
    "        self.update_center()\n",
    "        \n",
    "    def update_center(self):\n",
    "        \"\"\"\n",
    "        Recaculate the center of the centroid after receiving a new box.\n",
    "        \"\"\"\n",
    "        self.center = ((self.box[0][0]+self.box[1][0])//2, (self.box[0][1]+self.box[1][1])//2)\n",
    "\n",
    "    def near(self, point):\n",
    "        \"\"\"\n",
    "        Check if a point falls in the range of this centroid.\n",
    "        \n",
    "        Uses Euclidean distance.\n",
    "        \"\"\"\n",
    "        distance = ((self.center[0]-point[0])**2 + (self.center[1]-point[1])**2)**0.5\n",
    "        return distance <= self.MAX_PIXEL_DISTANCE\n",
    "    \n",
    "    def update_box(self, new_box):\n",
    "        \"\"\"\n",
    "        Update the bounding box of the object with a new observation.\n",
    "        \"\"\"\n",
    "        # take the average of the current box position with it's new position\n",
    "        self.draw_box = (((self.box[0][0]+new_box[0][0])//2, (self.box[0][1]+new_box[0][1])//2),\n",
    "                        ((self.box[1][0]+new_box[1][0])//2, (self.box[1][1]+new_box[1][1])//2))\n",
    "        self.previous_box = self.box\n",
    "        self.box = new_box\n",
    "        self.update_center()\n",
    "        \n",
    "    def set_active(self):\n",
    "        \"\"\"\n",
    "        Mark centroid as active at this time step.\n",
    "        \"\"\"\n",
    "        self.activations.append(True)\n",
    "        \n",
    "    def set_inactive(self):\n",
    "        \"\"\"\n",
    "        Mark centroid as inactive (not detected) at this time step.\n",
    "        \"\"\"\n",
    "        self.activations.append(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection entry point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Detector:\n",
    "    \"\"\"\n",
    "    Detector is the entry class for car detection.\n",
    "    \n",
    "    It keeps a running heatmap of detections in the image, \n",
    "    as well as a set of centroids for all detected objects.\n",
    "    \"\"\"\n",
    "    MAX_INACTIVITY = 60 # max number of frames without activity\n",
    "    MIN_ACTIVITY_COUNT = 3 # min number of activations before drawing\n",
    "    \n",
    "    def __init__(self, shape, classifiers, scaler):\n",
    "        self.heatmap = RunningHeatmap(shape)\n",
    "        self.classifiers = classifiers\n",
    "        self.scaler = scaler\n",
    "        self.centroids = set()\n",
    "        \n",
    "    def process_frame(self, image):\n",
    "        \"\"\"\n",
    "        Given the next frame in a video returns the same image with\n",
    "        boxes drawn around all detected cars.\n",
    "        \"\"\"\n",
    "        # get sliding windows\n",
    "        windows = slide_window(image, x_start_stop=[400, None], xy_window=(112, 112), xy_overlap=(0.6, 0.6))\n",
    "        # search all windows for cars and get the 'hot' windows with a positive detection\n",
    "        hot_windows = search_windows(image, windows, self.classifiers, self.scaler)\n",
    "        # update the running heatmap with the latest hot windows\n",
    "        self.heatmap.add(hot_windows)\n",
    "        \n",
    "        # get a labeled array from the heatmap, merging neighboring pixels which are\n",
    "        # considered to overlap as part of a single object with multiple hot windows\n",
    "        labels = label(self.heatmap.heatmap)\n",
    "        boxes = []\n",
    "        centroid_pts = []\n",
    "        for i in range(1, labels[1]+1):\n",
    "            nonzeroy, nonzerox = (labels[0] == i).nonzero()\n",
    "            box = ((np.min(nonzerox), np.min(nonzeroy)), (np.max(nonzerox), np.max(nonzeroy)))\n",
    "            centroid_pts.append(((box[0][0]+box[1][0])//2, (box[0][1]+box[1][1])//2))\n",
    "            boxes.append(box)\n",
    "        \n",
    "        found_centroids = set()\n",
    "        \n",
    "        # loop through all detected boxes and their centroid points\n",
    "        # and allocate points to current centroid objects if they fall in range\n",
    "        for i, cpt in enumerate(centroid_pts):\n",
    "            found = False\n",
    "            for centroid in self.centroids:\n",
    "                if centroid.near(cpt):\n",
    "                    centroid.set_active()\n",
    "                    centroid.update_box(boxes[i])\n",
    "                    found = True\n",
    "                    found_centroids.add(centroid)\n",
    "            \n",
    "            # if no capturing centroid object found, create a new one\n",
    "            if not found:\n",
    "                new_centroid = Centroid(boxes[i])\n",
    "                self.centroids.add(new_centroid)\n",
    "                found_centroids.add(new_centroid)\n",
    "                \n",
    "        # clean up old detections\n",
    "        dead_centroids = set()\n",
    "        for centroid in self.centroids - found_centroids:\n",
    "            centroid.set_inactive()\n",
    "            if len(centroid.activations) > self.MAX_INACTIVITY and not any(centroid.activations[-self.MAX_INACTIVITY:]):\n",
    "                dead_centroids.add(centroid)\n",
    "        \n",
    "        for c in dead_centroids:\n",
    "            self.centroids.remove(c)\n",
    "        \n",
    "        # draw boxes for objects with the minimum number of positive detections\n",
    "        for centroid in self.centroids:\n",
    "            if np.sum(centroid.activations) >= self.MIN_ACTIVITY_COUNT:\n",
    "                cv2.rectangle(image, centroid.draw_box[0], centroid.draw_box[1], (0,0,255), 6)\n",
    "        \n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection on project video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video submission.mp4\n",
      "[MoviePy] Writing video submission.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1260/1261 [12:24<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: submission.mp4 \n",
      "\n",
      "CPU times: user 1h 52s, sys: 1min 8s, total: 1h 2min 1s\n",
      "Wall time: 12min 24s\n"
     ]
    }
   ],
   "source": [
    "detector = Detector((720, 1280), [svc, booster], scaler)\n",
    "output_video = 'submission.mp4'\n",
    "input_clip = VideoFileClip(\"project_video.mp4\")\n",
    "output_clip = input_clip.fl_image(detector.process_frame)\n",
    "%time output_clip.write_videofile(output_video, audio=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
